{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Urban Driver\n",
    "\n",
    "In this notebook you are going to train the planner introduced in [Urban Driver: Learning to Drive from Real-world Demonstrations Using Policy Gradients](https://openreview.net/pdf?id=ibktAcINCaj).\n",
    "\n",
    "\n",
    "![model](../../docs/images/planning/TODO)\n",
    "\n",
    "You will train your model using the Lyft Prediction Dataset and [L5Kit](https://github.com/lyft/l5kit).\n",
    "**Before starting, please download the [Lyft L5 Prediction Dataset 2020](https://self-driving.lyft.com/level5/prediction/) and follow [the instructions](https://github.com/lyft/l5kit#download-the-datasets) to correctly organise it.**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Inputs\n",
    "TODO\n",
    "\n",
    "#### Outputs\n",
    "TODO\n",
    "\n",
    "### Training in closed loop\n",
    "TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import ChunkedDataset, LocalDataManager\n",
    "from l5kit.dataset import EgoDatasetVectorized\n",
    "from l5kit.planning.vectorized.closed_loop_model import VectorizedUnrollModel\n",
    "from l5kit.vectorization.vectorizer_builder import build_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download L5 Sample Dataset and install L5Kit\n",
    "import os\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !wget https://raw.githubusercontent.com/lyft/l5kit/master/examples/setup_notebook_colab.sh -q\n",
    "    !sh ./setup_notebook_colab.sh\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = open(\"./dataset_dir.txt\", \"r\").read().strip()\n",
    "else:\n",
    "    print(\"Not running in Google Colab.\")\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = \"/tmp/l5kit_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = LocalDataManager(None)\n",
    "# get config\n",
    "cfg = load_config_data(\"./config_clt.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== INIT DATASET\n",
    "train_zarr = ChunkedDataset(dm.require(cfg[\"train_data_loader\"][\"key\"])).open()\n",
    "\n",
    "vectorizer = build_vectorizer(cfg, dm)\n",
    "train_dataset = EgoDatasetVectorized(cfg, train_zarr, vectorizer)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_scaling = [1.0, 1.0, 1.0]\n",
    "\n",
    "_num_predicted_frames = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "_num_predicted_params = len(weights_scaling)\n",
    "\n",
    "model = VectorizedUnrollModel(\n",
    "    history_num_frames_ego=cfg[\"model_params\"][\"history_num_frames_ego\"],\n",
    "    history_num_frames_agents=cfg[\"model_params\"][\"history_num_frames_agents\"],\n",
    "    num_targets=_num_predicted_params * _num_predicted_frames,\n",
    "    weights_scaling=weights_scaling,\n",
    "    criterion=nn.L1Loss(reduction=\"none\"),\n",
    "    disable_other_agents=cfg[\"model_params\"][\"disable_other_agents\"],\n",
    "    disable_map=cfg[\"model_params\"][\"disable_map\"],\n",
    "    disable_lane_boundaries=cfg[\"model_params\"][\"disable_lane_boundaries\"],\n",
    "    detach_unroll=cfg[\"model_params\"][\"detach_unroll\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for training\n",
    "Our `EgoDatasetVectorized` inherits from PyTorch `Dataset`; so we can use it inside a `Dataloader` to enable multi-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cfg = cfg[\"train_data_loader\"]\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=train_cfg[\"shuffle\"], batch_size=train_cfg[\"batch_size\"],\n",
    "                              num_workers=train_cfg[\"num_workers\"])\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "Here, we purposely include a barebone training loop. Clearly, many more components can be added to enrich logging and improve performance. Still, the sheer size of our dataset ensures that a reasonable performance can be obtained even with this simple loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_it = iter(train_dataloader)\n",
    "progress_bar = tqdm(range(cfg[\"train_params\"][\"max_num_steps\"]))\n",
    "losses_train = []\n",
    "model.train()\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "for _ in progress_bar:\n",
    "    try:\n",
    "        data = next(tr_it)\n",
    "    except StopIteration:\n",
    "        tr_it = iter(train_dataloader)\n",
    "        data = next(tr_it)\n",
    "    # Forward pass\n",
    "    data = {k: v.to(device) for k, v in data.items()}\n",
    "    result = model(data)\n",
    "    loss = result[\"loss\"]\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses_train.append(loss.item())\n",
    "    progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the train loss curve\n",
    "We can plot the train loss against the iterations (batch-wise) to check if our model has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store the model\n",
    "\n",
    "Let's store the model as a torchscript. This format allows us to re-load the model and weights without requiring the class definition later.\n",
    "\n",
    "**Take note of the path, you will use it later to evaluate your planning model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "to_save = torch.jit.script(model.cpu())\n",
    "path_to_save = f\"{gettempdir()}/urban_planner.pt\"\n",
    "to_save.save(path_to_save)\n",
    "print(f\"MODEL STORED at {path_to_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations in training your very own Urban Planner model!\n",
    "### What's Next\n",
    "\n",
    "Now that your model is trained and safely stored, you can evaluate how it performs in two very different situations using our dedicated notebooks:\n",
    "\n",
    "# TODO can we reuse these notebooks actually??\n",
    "\n",
    "### [Open-loop evaluation](./open_loop_test.ipynb)\n",
    "In this setting the model **is not controlling the AV**, and predictions are used to compute metrics only.\n",
    "\n",
    "### [Closed-loop evaluation](./closed_loop_test.ipynb)\n",
    "In this setting the model **is in full control of the AV** future movements.\n",
    "\n",
    "## Pre-trained models\n",
    "we provide a collection of pre-trained models for the planning task:\n",
    "TODO\n",
    "\n",
    "To use one of the models simply download the corresponding `.pt` file and load it in the evaluation notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
