

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Coordinate Systems in L5Kit &mdash; L5Kit 1.0.5 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> L5Kit
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="README.html">ML Prediction, Planning and Simulation for Self-Driving</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#examples">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#news">News</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#overview">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#license">License</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#credits">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="README.html#contact">Contact</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_reference.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_format.html">Dataset Formats</a></li>
<li class="toctree-l1"><a class="reference internal" href="how_to_contribute.html">How to contribute</a></li>
<li class="toctree-l1"><a class="reference internal" href="competition.html">Competition</a></li>
<li class="toctree-l1"><a class="reference internal" href="competition.html#scoring">Scoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="competition.html#additional-metrics">Additional Metrics</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">L5Kit</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Coordinate Systems in L5Kit</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/coords_systems.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="coordinate-systems-in-l5kit">
<h1>Coordinate Systems in L5Kit<a class="headerlink" href="#coordinate-systems-in-l5kit" title="Permalink to this headline">¶</a></h1>
</div>
<div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>One essential feature of L5Kit is converting raw data into multi-channel images. We refer to this process as
<strong>rasterisation</strong>. These raw data can come from different sources, such as:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">.zarr</span></code> datasets, which include information about the AV and other agents;</p></li>
<li><p>static aerial images;</p></li>
<li><p>dynamic semantic maps, which include lane topologies, crosswalks, etc..</p></li>
</ul>
<p>Clearly, these different sources can have different coordinates systems, which need to be converted to a single common system
before we can use them together to get our final multi-channel image.
L5Kit performs this operation smoothly under the hood, but you may want to know more details about these different systems
if you’re trying a more experimental workflow.</p>
</div>
<div class="section" id="coordinate-systems">
<h1>Coordinate Systems<a class="headerlink" href="#coordinate-systems" title="Permalink to this headline">¶</a></h1>
<div class="section" id="world-coordinate-system">
<h2>World Coordinate System<a class="headerlink" href="#world-coordinate-system" title="Permalink to this headline">¶</a></h2>
<p>We refer to the coordinate system in the <code class="docutils literal notranslate"><span class="pre">.zarr</span></code> dataset as <strong>world</strong>. This is shared by <em>all</em> our zarrs in a dataset
and is a 3D metric space. The entities living in this space are the AV and the agents:</p>
<ul class="simple">
<li><p>AV: samples from the AV are collected using several sensors placed in the car. The AV translation is a 3D vector (XYZ),
while the orientation is expressed as a 3x3 rotation matrix (counterclockwise in a right-hand system).</p></li>
<li><p>Agents: samples from other agents are generated while the AV moves around. The translation is a 2D vector (XY) and the orientation
is expressed via a single <a class="reference external" href="https://en.wikipedia.org/wiki/Yaw_(rotation)">yaw angle</a> (counterclockwise in radians).</p></li>
</ul>
<p>The origin of the <strong>world</strong> coordinate system is located at <a class="reference external" href="https://www.google.com/maps/place/37%C2%B025%2745.6%22N+122%C2%B009%2715.7%22W/&#64;37.4293427,-122.1565407">[37°25’45.6”N, 122°09’15.7”W]</a> in Palo Alto (California, USA).</p>
</div>
<div class="section" id="image-coordinate-system">
<h2>Image Coordinate System<a class="headerlink" href="#image-coordinate-system" title="Permalink to this headline">¶</a></h2>
<p>Once rasterisation is complete, the final multi-channel image will be in the image space. This is a 2D space where (0,0)
is located in the top-left corner.</p>
<p>If you’re using one of our high-level dataset objects (either <code class="docutils literal notranslate"><span class="pre">EgoDataset</span></code> or <code class="docutils literal notranslate"><span class="pre">AgentDataset</span></code>) to generate samples, you can
access the world-to-image matrix using the <code class="docutils literal notranslate"><span class="pre">world_to_image</span></code> key on the returned dict. When building this matrix, several steps are combined:</p>
<ul class="simple">
<li><p>Translate world to ego by applying the negative <code class="docutils literal notranslate"><span class="pre">ego_translation</span></code>;</p></li>
<li><p>Rotate counter-clockwise by negative <code class="docutils literal notranslate"><span class="pre">ego_yaw</span></code> to align world such that ego faces right in the image;</p></li>
<li><p>Scale from meters to pixels based on the value <code class="docutils literal notranslate"><span class="pre">pixel_size</span></code> set in the configuration;</p></li>
<li><p>Translate again such that the ego is aligned to the value <code class="docutils literal notranslate"><span class="pre">ego_center</span></code> in the configuration.</p></li>
</ul>
<p>Note: we ignore the z coordinate in this transformation</p>
<p>With this matrix, you can transform a point from world to image space and vice versa using its inverse.</p>
<p>One application of this is drawing trajectories on the image:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># this comes from either EgoDataset or AgentDataset</span>

<span class="n">im</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">rasterizer</span><span class="o">.</span><span class="n">to_rgb</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;image&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>  <span class="c1"># convert raster into rgb</span>

<span class="c1"># transform from world meters into image pixels</span>
<span class="n">positions_pixels</span> <span class="o">=</span> <span class="n">transform_points</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;target_positions&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;centroid&quot;</span><span class="p">][:</span><span class="mi">2</span><span class="p">],</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;world_to_image&quot;</span><span class="p">])</span>
<span class="n">draw_trajectory</span><span class="p">(</span><span class="n">im</span><span class="p">,</span> <span class="n">positions_pixels</span><span class="p">,</span> <span class="n">data</span><span class="p">[</span><span class="s2">&quot;target_yaws&quot;</span><span class="p">],</span> <span class="n">TARGET_POINTS_COLOR</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="satellite-coordinate-system">
<h2>Satellite Coordinate System<a class="headerlink" href="#satellite-coordinate-system" title="Permalink to this headline">¶</a></h2>
<p>Satellite information is stored as an RGB image in the <code class="docutils literal notranslate"><span class="pre">aerial_map</span></code> folder of the dataset. Together with that we provide
a matrix to convert from the <a class="reference external" href="https://en.wikipedia.org/wiki/ECEF">ECEF</a> reference system to this image reference system (i.e. converting XYZ ECEF coordinates into a 2D pixel coordinates).</p>
<p>However, the <code class="docutils literal notranslate"><span class="pre">.zarr</span></code> stores information in the world reference system. As such, an additional conversion is required from world to this image reference system:</p>
<ul class="simple">
<li><p>world coordinates must be converted into ECEF coordinates. This transformation matrix is currently hard-coded but will be shipped with the dataset
in the future. It is <strong>dataset dependent</strong> as it encodes where the dataset world origin is located in the Earth frame;</p></li>
<li><p>ECEF coordinates must be converted into the aerial image reference system using the above mentioned matrix.</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">SatelliteRasterizer</span></code> and its derived classes combine these two matrices into a single one and directly convert
world coordinates from the <code class="docutils literal notranslate"><span class="pre">.zarr</span></code> into 2D pixels coordinates. In this way, you can rasterise around an agent in the <code class="docutils literal notranslate"><span class="pre">.zarr</span></code> (whose coordinates are in the world reference system)</p>
</div>
<div class="section" id="semantic-coordinate-system">
<h2>Semantic Coordinate System<a class="headerlink" href="#semantic-coordinate-system" title="Permalink to this headline">¶</a></h2>
<p>Semantic information is stored as a protobuf file. The protobuf store information as a list of elements of different types (e.g lanes, crosswalks, etc).</p>
<p>Each elements can have one or multiple geometric features (e.g. the left and right lane boundaries) which are described
as a list of 3D points.</p>
<p>Each element’s features are localised in its local coordinate system:</p>
<ul class="simple">
<li><p>features coordinates are expressed in centimeters deltas in an <a class="reference external" href="https://en.wikipedia.org/wiki/Local_tangent_plane_coordinates">ENU</a> reference system. This system is valid <strong>only</strong> for that feature
(i.e. two features with the same coordinates values are <strong>not</strong> in the same location);</p></li>
<li><p>the system latitude and longitude, which localise the feature in a global reference system.
This can be used for example to move the feature into a common space (e.g. world or ECEF)</p></li>
</ul>
<p>The <code class="docutils literal notranslate"><span class="pre">MapAPI</span></code> class has a set of functionality to convert these local spaces into a global reference system.
When you query it for a supported element (only lanes and crosswalks currently):</p>
<ul class="simple">
<li><p>the geometric feature(s) is converted from deltas to absolute values;</p></li>
<li><p>the feature is then converted from ENU into ECEF by using its GeoFrame reference (lat, lng);</p></li>
<li><p>the features is finally converted from ECEF to world (this is the reason for the <code class="docutils literal notranslate"><span class="pre">world_to_ecef</span></code> arg in the <code class="docutils literal notranslate"><span class="pre">MapAPI</span></code> constructor).</p></li>
</ul>
<p>As these operations are computationally expensive, the function results are LRUcached</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Lyft Level 5

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>